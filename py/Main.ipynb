{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "\n",
    "import os\n",
    "PATH = 'F:/dataset'#数据集目录地址\n",
    "datasets = ['train','test']\n",
    "train_files = ['NORMAL','B','OR','IR']\n",
    "train_file_nums = [2,6,14,6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:43: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(train_file_nums[0]):\n",
    "    tmp_data = pd.read_csv(os.path.join(PATH, 'train', train_files[0]+str(i+1)+\".csv\"))\n",
    "    #tmp_data = pd.read_csv(os.path.join(DATA_DIR_PATH, datasets[0], train_files[0]+\"0\"+str(i+1)+\".csv\"))\n",
    "    #如果没有去掉文件名中的0则应使用该行代码，下同 \n",
    "    tmp_data['label'] = 0\n",
    "    tmp_data['file'] = i+1\n",
    "    if i == 0:\n",
    "        normal_data = tmp_data\n",
    "    else:\n",
    "        normal_data = pd.concat([normal_data, tmp_data], axis = 0)\n",
    "        \n",
    "for i in range(train_file_nums[1]):\n",
    "    tmp_data = pd.read_csv(os.path.join(PATH, 'train', train_files[1]+str(i+1)+\".csv\"))\n",
    "    tmp_data['label'] = 1\n",
    "    tmp_data['file'] = i+1\n",
    "    if i == 0:\n",
    "        ball_data = tmp_data\n",
    "    else:\n",
    "        ball_data = pd.concat([ball_data, tmp_data], axis = 0)\n",
    "        \n",
    "for i in range(train_file_nums[2]):\n",
    "    if i >= 9:\n",
    "        filename = train_files[2]+str(i+1)+\".csv\"\n",
    "    else:\n",
    "        filename = train_files[2]+str(i+1)+\".csv\"\n",
    "    tmp_data = pd.read_csv(os.path.join(PATH, 'train', filename))\n",
    "    tmp_data['label'] = 2\n",
    "    tmp_data['file'] = i+1\n",
    "    if i == 0:\n",
    "        or_data = tmp_data\n",
    "    else:\n",
    "        or_data = pd.concat([or_data, tmp_data], axis = 0)\n",
    "        \n",
    "for i in range(train_file_nums[3]):\n",
    "    tmp_data = pd.read_csv(os.path.join(PATH,'train', train_files[3]+str(i+1)+\".csv\"))\n",
    "    tmp_data['label'] = 3\n",
    "    tmp_data['file'] = i+1\n",
    "    if i == 0:\n",
    "        ir_data = tmp_data\n",
    "    else:\n",
    "        ir_data = pd.concat([ir_data, tmp_data], axis = 0)\n",
    "        \n",
    "train_data = pd.concat([normal_data,ball_data,or_data,ir_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats,signal,fftpack\n",
    "from pywt import wavedec\n",
    "\"\"\"特征提取\n",
    " 时域：\n",
    " 均值，标准差，最大值，最小值，均方根，峰峰值，中位数，四分位差，百分位差，\n",
    " 偏度，峰度，方差，整流平均值，方根幅值，波形因子，峰值因子，脉冲值，裕度\n",
    " 频域：\n",
    " 均值，标准差，最大值，最小值，均方根，中位数，四分位差，百分位差\n",
    " f2 f3 f4反映频谱集中程度\n",
    " f5 f6 f7 f8反映主频带位置\n",
    " 5级小波变换，最后输出6个能量特征和其归一化能量特征\"\"\"\n",
    "    \n",
    "columns = ['time_mean','time_std','time_max','time_min','time_rms','time_ptp','time_median',\n",
    "                                     'time_iqr','time_pr','time_skew','time_kurtosis','time_var','time_amp',\n",
    "                                     'time_smr','time_wavefactor','time_peakfactor','time_pulse','time_margin',\n",
    "                                     'freq_mean','freq_std','freq_max','freq_min','freq_rms','freq_median',\n",
    "                                     'freq_iqr','freq_pr','freq_f2','freq_f3','freq_f4','freq_f5','freq_f6','freq_f7',\n",
    "                                     'freq_f8','ener_cA5','ener_cD1','ener_cD2','ener_cD3','ener_cD4','ener_cD5',\n",
    "                                     'ratio_cA5','ratio_cD1','ratio_cD2','ratio_cD3','ratio_cD4','ratio_cD5']\n",
    "data_names = ['DE_','FE_']\n",
    "new_columns = []\n",
    "for data_name in data_names:\n",
    "    new_columns.extend([data_name+j for j in columns])\n",
    "\n",
    "\n",
    "def featuring(this_file_data,windowlen):\n",
    "    n_line = this_file_data.shape[0]\n",
    "    features = []\n",
    "    for i in range (0,n_line,windowlen):\n",
    "        if (n_line - i)/windowlen >= 1:\n",
    "            df = this_file_data.loc[i:i+windowlen,['DE_time','FE_time']]\n",
    "            feature_list = []\n",
    "            for col in df.columns:\n",
    "                df_line = df[col]\n",
    "                time_mean = df_line.mean()                                           \n",
    "                time_std = df_line.std()\n",
    "                time_max = df_line.max()\n",
    "                time_min = df_line.min()\n",
    "                time_rms = np.sqrt(np.square(df_line).mean())\n",
    "                time_ptp = time_max-time_min \n",
    "                time_median = np.median(df_line)\n",
    "                time_iqr = np.percentile(df_line,75)-np.percentile(df_line,25)\n",
    "                time_pr = np.percentile(df_line,90)-np.percentile(df_line,10)\n",
    "                time_skew = stats.skew(df_line)\n",
    "                time_kurtosis = stats.kurtosis(df_line)\n",
    "                time_var = np.var(df_line)\n",
    "                time_amp = np.abs(df_line).mean()\n",
    "                time_smr = np.square(np.sqrt(np.abs(df_line)).mean())\n",
    "#               下面四个特征需要注意分母为0或接近0问题，可能会发生报错\n",
    "                time_wavefactor = time_rms/time_amp\n",
    "                time_peakfactor = time_max/time_rms\n",
    "                time_pulse = time_max/time_amp\n",
    "                time_margin = time_max/time_smr\n",
    "#               采样频率25600Hz\n",
    "                df_fftline = fftpack.fft(df[col])\n",
    "                freq_fftline = fftpack.fftfreq(len(df[col]),1/25600)\n",
    "                df_fftline = abs(df_fftline[freq_fftline>=0])\n",
    "                freq_fftline = freq_fftline[freq_fftline>=0]\n",
    "                freq_mean = df_fftline.mean()\n",
    "                freq_std = df_fftline.std()\n",
    "                freq_max = df_fftline.max()\n",
    "                freq_min = df_fftline.min()\n",
    "                freq_rms = np.sqrt(np.square(df_fftline).mean())\n",
    "                freq_median = np.median(df_fftline)\n",
    "                freq_iqr = np.percentile(df_fftline,75)-np.percentile(df_fftline,25)\n",
    "                freq_pr = np.percentile(df_fftline,90)-np.percentile(df_fftline,10)\n",
    "                freq_f2 = np.square((df_fftline-freq_mean)).sum()/(len(df_fftline)-1)\n",
    "                freq_f3 = pow((df_fftline-freq_mean),3).sum()/(len(df_fftline)*pow(freq_f2,1.5))\n",
    "                freq_f4 = pow((df_fftline-freq_mean),4).sum()/(len(df_fftline)*pow(freq_f2,2))\n",
    "                freq_f5 = np.multiply(freq_fftline,df_fftline).sum()/df_fftline.sum()\n",
    "                freq_f6 = np.sqrt(np.multiply(np.square(freq_fftline),df_fftline).sum())/df_fftline.sum()\n",
    "                freq_f7 = np.sqrt(np.multiply(pow(freq_fftline,4),df_fftline).sum())/np.multiply(np.square(freq_fftline),df_fftline).sum()\n",
    "                freq_f8 = np.multiply(np.square(freq_fftline),df_fftline).sum()/np.sqrt(np.multiply(pow(freq_fftline,4),df_fftline).sum()*df_fftline.sum())\n",
    "                #----------  timefreq-domain feature,12\n",
    "                # 5级小波变换，最后输出6个能量特征和其归一化能量特征\n",
    "                cA5, cD5, cD4, cD3, cD2, cD1 = wavedec(df[col], 'db10', level=5)\n",
    "                ener_cA5 = np.square(cA5).sum()\n",
    "                ener_cD5 = np.square(cD5).sum()\n",
    "                ener_cD4 = np.square(cD4).sum()\n",
    "                ener_cD3 = np.square(cD3).sum()\n",
    "                ener_cD2 = np.square(cD2).sum()\n",
    "                ener_cD1 = np.square(cD1).sum()\n",
    "                ener = ener_cA5 + ener_cD1 + ener_cD2 + ener_cD3 + ener_cD4 + ener_cD5\n",
    "                ratio_cA5 = ener_cA5/ener\n",
    "                ratio_cD5 = ener_cD5/ener\n",
    "                ratio_cD4 = ener_cD4/ener\n",
    "                ratio_cD3 = ener_cD3/ener\n",
    "                ratio_cD2 = ener_cD2/ener\n",
    "                ratio_cD1 = ener_cD1/ener\n",
    "\n",
    "                feature_list.extend([time_mean,time_std,time_max,time_min,time_rms,time_ptp,time_median,\n",
    "                                     time_iqr,time_pr,time_skew,time_kurtosis,time_var,time_amp,\n",
    "                                     time_smr,time_wavefactor,time_peakfactor,time_pulse,time_margin,\n",
    "                                     freq_mean,freq_std,freq_max,freq_min,freq_rms,freq_median,\n",
    "                                     freq_iqr,freq_pr,freq_f2,freq_f3,freq_f4,freq_f5,freq_f6,freq_f7,\n",
    "                                     freq_f8,ener_cA5,ener_cD1,ener_cD2,ener_cD3,ener_cD4,ener_cD5,\n",
    "                                     ratio_cA5,ratio_cD1,ratio_cD2,ratio_cD3,ratio_cD4,ratio_cD5])\n",
    "            features.append(feature_list)\n",
    "    features = np.asarray(features)\n",
    "    return features\n",
    "\n",
    "\n",
    "def Trainfeaturing(train_files,train_data,windowlen):\n",
    "    flag = 0\n",
    "    for label in range(len(train_files)):\n",
    "        this_label_data = train_data[train_data.label == label]\n",
    "        n_file = max(this_label_data.file)\n",
    "        for file_index in range(1,n_file+1):\n",
    "            this_file_data = this_label_data[this_label_data.file == file_index]\n",
    "            if flag == 0:\n",
    "                features = featuring(this_file_data,windowlen)\n",
    "                flag += 1\n",
    "                labels = [label for j in range(features.shape[0])]\n",
    "            else:\n",
    "                new_features = featuring(this_file_data,windowlen)\n",
    "                features = np.concatenate((features, new_features), axis=0)\n",
    "                labels.extend([label for j in range(new_features.shape[0])])\n",
    "    features = np.asarray(features)\n",
    "    labels = np.asarray(labels)\n",
    "    return features,labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file = 'TEST'\n",
    "test_file_num = 142\n",
    "windowlen = 1750\n",
    "\n",
    "features,labels = Trainfeaturing(train_files,train_data,windowlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['time_mean','time_std','time_max','time_min','time_rms','time_ptp','time_median',\n",
    "                                     'time_iqr','time_pr','time_skew','time_kurtosis','time_var','time_amp',\n",
    "                                     'time_smr','time_wavefactor','time_peakfactor','time_pulse','time_margin',\n",
    "                                     'freq_mean','freq_std','freq_max','freq_min','freq_rms','freq_median',\n",
    "                                     'freq_iqr','freq_pr','freq_f2','freq_f3','freq_f4','freq_f5','freq_f6','freq_f7',\n",
    "                                     'freq_f8','ener_cA5','ener_cD1','ener_cD2','ener_cD3','ener_cD4','ener_cD5',\n",
    "                                     'ratio_cA5','ratio_cD1','ratio_cD2','ratio_cD3','ratio_cD4','ratio_cD5']\n",
    "data_names = ['DE_','FE_']\n",
    "new_columns = []\n",
    "for data_name in data_names:\n",
    "    new_columns.extend([data_name+j for j in columns])\n",
    "fea_table = pd.DataFrame(features,columns=new_columns)\n",
    "fea_table['label'] = labels\n",
    "fea_table.to_csv(os.path.join(PATH,'trainresult.csv'),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection  import train_test_split\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(features, labels, train_size=0.9, random_state=10010)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 评分规则，f1-score 权重 ball inner outer 各占0.3 normal 0.1\n",
    "from sklearn.metrics import f1_score,confusion_matrix\n",
    "def get_final_score(real,predict,mat=False,score=False):\n",
    "    # 共有4类\n",
    "    final_score = 0\n",
    "    if mat:\n",
    "        mat = confusion_matrix(real,predict)\n",
    "        print(mat)\n",
    "    for i in range(4):\n",
    "        real_i = (real == i)\n",
    "        # print(real_i)\n",
    "        predict_i = (predict == i)\n",
    "        # print(predict_i)\n",
    "        this_f1_score = f1_score(real_i, predict_i)\n",
    "        if score:\n",
    "            print(i,\":\",this_f1_score)\n",
    "        if i == 0:\n",
    "            this_f1_score = this_f1_score*0.1\n",
    "        else:\n",
    "            this_f1_score = this_f1_score*0.3\n",
    "        final_score += this_f1_score\n",
    "    return final_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "[3 3 0 0 0 1 0 0 0 0 0 3 0 0 3 0 0 2 1 0 0 1 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 3 0 2 2 0 2 0 0 0 3]\n",
      "[3 3 0 0 0 1 0 0 0 0 0 3 0 0 3 0 0 2 1 0 0 1 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 3 0 2 2 0 2 0 0 0 3]\n",
      "mean_score 1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['cwru.model']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "params = {  \n",
    " 'boosting_type': 'gbdt',  \n",
    "    'objective': 'multiclass',  \n",
    "    'num_class': 4,  \n",
    "    'metric': 'multi_error',  \n",
    "    'num_leaves': 200,  \n",
    "    'min_data_in_leaf': 40,  \n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.85,  \n",
    "    'bagging_fraction': 0.85,  \n",
    "    'bagging_freq':10,  \n",
    "    'lambda_l1': 0.1,  \n",
    "    'lambda_l2': 0.1,  \n",
    "    'min_gain_to_split': 0.2,  \n",
    "    'verbose': -1, \n",
    "    'is_unbalance':'true'\n",
    "}  \n",
    "\n",
    "print('Training...')\n",
    "trn_data = lgb.Dataset(X_train, Y_train)\n",
    "\n",
    "clf = lgb.train(params, \n",
    "                trn_data ,\n",
    "                num_boost_round = 1000,\n",
    "                #valid_sets = [X_train,X_test], \n",
    "                verbose_eval = 100, \n",
    "                #early_stopping_rounds = 100\n",
    "               )\n",
    "\n",
    "\n",
    "y_prob = clf.predict(X_test, num_iteration=clf.best_iteration)\n",
    "y_pred = [list(x).index(max(x)) for x in y_prob]\n",
    "y_pred=np.array(y_pred)\n",
    "\n",
    "print(Y_test)\n",
    "print(y_pred)\n",
    "\n",
    "scores = []\n",
    "\n",
    "scores.append(get_final_score(Y_test,y_pred))\n",
    "scores = np.asarray(scores)\n",
    "print(\"mean_score\",scores.mean())\n",
    "\n",
    "import joblib\n",
    "joblib.dump(clf,'cwru.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting...\n",
      "file 1\n",
      "0\n",
      "file 2\n",
      "0\n",
      "file 3\n",
      "3\n",
      "file 4\n",
      "1\n",
      "file 5\n",
      "0\n",
      "file 6\n",
      "0\n",
      "file 7\n",
      "0\n",
      "file 8\n",
      "2\n",
      "file 9\n",
      "0\n",
      "file 10\n",
      "1\n",
      "file 11\n",
      "0\n",
      "file 12\n",
      "0\n",
      "file 13\n",
      "0\n",
      "file 14\n",
      "0\n",
      "file 15\n",
      "0\n",
      "file 16\n",
      "0\n",
      "file 17\n",
      "3\n",
      "file 18\n",
      "0\n",
      "file 19\n",
      "0\n",
      "file 20\n",
      "0\n",
      "file 21\n",
      "1\n",
      "file 22\n",
      "2\n",
      "file 23\n",
      "0\n",
      "file 24\n",
      "0\n",
      "file 25\n",
      "0\n",
      "file 26\n",
      "0\n",
      "file 27\n",
      "0\n",
      "file 28\n",
      "0\n",
      "file 29\n",
      "0\n",
      "file 30\n",
      "0\n",
      "file 31\n",
      "0\n",
      "file 32\n",
      "0\n",
      "file 33\n",
      "0\n",
      "file 34\n",
      "0\n",
      "file 35\n",
      "0\n",
      "file 36\n",
      "3\n",
      "file 37\n",
      "3\n",
      "file 38\n",
      "0\n",
      "file 39\n",
      "0\n",
      "file 40\n",
      "0\n",
      "file 41\n",
      "0\n",
      "file 42\n",
      "2\n",
      "file 43\n",
      "0\n",
      "file 44\n",
      "0\n",
      "file 45\n",
      "0\n",
      "file 46\n",
      "0\n",
      "file 47\n",
      "0\n",
      "file 48\n",
      "0\n",
      "file 49\n",
      "0\n",
      "file 50\n",
      "0\n",
      "file 51\n",
      "0\n",
      "file 52\n",
      "0\n",
      "file 53\n",
      "0\n",
      "file 54\n",
      "2\n",
      "file 55\n",
      "0\n",
      "file 56\n",
      "0\n",
      "file 57\n",
      "0\n",
      "file 58\n",
      "0\n",
      "file 59\n",
      "0\n",
      "file 60\n",
      "1\n",
      "file 61\n",
      "0\n",
      "file 62\n",
      "1\n",
      "file 63\n",
      "0\n",
      "file 64\n",
      "0\n",
      "file 65\n",
      "3\n",
      "file 66\n",
      "0\n",
      "file 67\n",
      "2\n",
      "file 68\n",
      "2\n",
      "file 69\n",
      "1\n",
      "file 70\n",
      "0\n",
      "file 71\n",
      "2\n",
      "file 72\n",
      "0\n",
      "file 73\n",
      "0\n",
      "file 74\n",
      "0\n",
      "file 75\n",
      "2\n",
      "file 76\n",
      "0\n",
      "file 77\n",
      "0\n",
      "file 78\n",
      "0\n",
      "file 79\n",
      "2\n",
      "file 80\n",
      "0\n",
      "file 81\n",
      "0\n",
      "file 82\n",
      "0\n",
      "file 83\n",
      "2\n",
      "file 84\n",
      "0\n",
      "file 85\n",
      "0\n",
      "file 86\n",
      "0\n",
      "file 87\n",
      "0\n",
      "file 88\n",
      "0\n",
      "file 89\n",
      "0\n",
      "file 90\n",
      "0\n",
      "file 91\n",
      "0\n",
      "file 92\n",
      "0\n",
      "file 93\n",
      "0\n",
      "file 94\n",
      "0\n",
      "file 95\n",
      "0\n",
      "file 96\n",
      "0\n",
      "file 97\n",
      "0\n",
      "file 98\n",
      "0\n",
      "file 99\n",
      "0\n",
      "file 100\n",
      "0\n",
      "file 101\n",
      "0\n",
      "file 102\n",
      "0\n",
      "file 103\n",
      "0\n",
      "file 104\n",
      "3\n",
      "file 105\n",
      "3\n",
      "file 106\n",
      "1\n",
      "file 107\n",
      "0\n",
      "file 108\n",
      "1\n",
      "file 109\n",
      "0\n",
      "file 110\n",
      "0\n",
      "file 111\n",
      "0\n",
      "file 112\n",
      "0\n",
      "file 113\n",
      "0\n",
      "file 114\n",
      "0\n",
      "file 115\n",
      "1\n",
      "file 116\n",
      "2\n",
      "file 117\n",
      "2\n",
      "file 118\n",
      "0\n",
      "file 119\n",
      "3\n",
      "file 120\n",
      "2\n",
      "file 121\n",
      "0\n",
      "file 122\n",
      "1\n",
      "file 123\n",
      "0\n",
      "file 124\n",
      "1\n",
      "file 125\n",
      "0\n",
      "file 126\n",
      "0\n",
      "file 127\n",
      "0\n",
      "file 128\n",
      "3\n",
      "file 129\n",
      "0\n",
      "file 130\n",
      "1\n",
      "file 131\n",
      "0\n",
      "file 132\n",
      "0\n",
      "file 133\n",
      "1\n",
      "file 134\n",
      "0\n",
      "file 135\n",
      "0\n",
      "file 136\n",
      "0\n",
      "file 137\n",
      "0\n",
      "file 138\n",
      "0\n",
      "file 139\n",
      "0\n",
      "file 140\n",
      "3\n",
      "file 141\n",
      "2\n",
      "file 142\n",
      "3\n",
      "succeed\n"
     ]
    }
   ],
   "source": [
    "m='/test/TEST'#/test为存储测试数据的文件夹名，TEST为文件名数字前的TEST\n",
    "a=PATH+m\n",
    "b='.csv'\n",
    "windowlen=1750\n",
    "\n",
    "print('Predicting...')\n",
    "import csv\n",
    "f = open(\"result.csv\", \"w\",encoding='utf-8',newline='') \n",
    "csv_writer = csv.writer(f)\n",
    "f.close()\n",
    "#清空文件\n",
    "f = open(\"result.csv\", \"a\",encoding='utf-8',newline='') \n",
    "csv_writer = csv.writer(f)\n",
    "csv_writer.writerow([\"filename\",\"label\"])\n",
    "for s in range(1,143):\n",
    "    print('file',s)\n",
    "    temp=a+str(s)+b\n",
    "    temp2=\"TEST\"+str(s)\n",
    "    params['testdata_path'] = str(temp)\n",
    "    data7 = pd.DataFrame(pd.read_csv(params['testdata_path']))\n",
    "    X_test = featuring(data7,1750)\n",
    "    y_prob = clf.predict(X_test, num_iteration=clf.best_iteration)\n",
    "\n",
    "    y_pred = [list(x).index(max(x)) for x in y_prob]\n",
    "    y_pred = str(y_pred)\n",
    "    \n",
    "    y_pred=y_pred.replace(\"[\", \"\");\n",
    "    y_pred=y_pred.replace(\"]\", \"\");\n",
    "    print(co(y_pred))\n",
    "    csv_writer = csv.writer(f)\n",
    "\n",
    "    csv_writer.writerow([temp2,co(y_pred)])\n",
    "f.close()\n",
    "print(\"succeed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
